{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "import pickle \n",
    "from sklearn.externals import joblib\n",
    "from scipy import sparse\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_df = DataSet.load_all_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate word-space tf-idf, max 3-gram\n",
    "space=\"words\"\n",
    "n=3\n",
    "vectorizer,ques_tfidf_embedding_array = Feature.get_ques_term_tfidf_matrix(question_df,space,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer,open(\"words3_3gram_vectorizer.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_v = pickle.load(open(\"words3_3gram_vectorizer.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz(\"words_3gram_sentence_tfidf_embedding.npz\", ques_tfidf_embedding_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your_matrix_back = sparse.load_npz(\"words_3gram_sentence_tfidf_embedding.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"words_3gram_tfidf_SVD_sentence_embed.np\",que_tfidf_svd_embedding_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_SVD_embedding.to_csv(\"words_3gram_tfidf_SVD_word_embed.csv\",header=False,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_and_store_intermediate_resutls():\n",
    "    all_questions = DataSet.load_all_questions()\n",
    "    spaces = [\"chars\"]\n",
    "    for space in spaces:\n",
    "        n = 3 if space==\"words\" else 5\n",
    "        print \"loading....\"\n",
    "        chars_5gram_vectorizer,ques_tfidf_embedding_array = Feature.get_ques_term_tfidf_matrix(all_questions,space,n)\n",
    "        pickle.dump(chars_5gram_vectorizer,open(\"chars_5gram_vectorizer.pkl\",\"wb\"))\n",
    "        sparse.save_npz(\"chars_5gram_sentence_tfidf_embedding.npz\", ques_tfidf_embedding_array)\n",
    "        que_tfidf_svd_embedding_array,term_SVD_embedding,svder = Feature.get_SVD_feature(chars_5gram_vectorizer,ques_tfidf_embedding_array,300)\n",
    "        ## Save SVD sentence embedding , term_embedding, and SVDer\n",
    "        np.save(\"chars_5gram_tfidf_SVD_sentence_embed\", que_tfidf_svd_embedding_array)\n",
    "        term_SVD_embedding.to_csv(\"chars_5gram_tfidf_SVD_char_embed.csv\", header=False, sep=\" \")\n",
    "        pickle.dump(svder,open(\"chars_5gram_tfidf_SVDer.pkl\",\"wb\"))\n",
    "        ## SAVE sentence composed of term_embedding\n",
    "        for i, term_embedding in enumerate([term_SVD_embedding, DataSet.load_term_embed(space)]):\n",
    "            for mode in [\"equally\", \"tf_idf_linear\", \"tf_idf_exp\"]:\n",
    "                question_emb_from_word_embed = Feature.get_question_embedding_from_term_embedding(all_questions,\n",
    "                                                                                                  term_embedding,\n",
    "                                                                                                  space,\n",
    "                                                                                                  chars_5gram_vectorizer,\n",
    "                                                                                                  ques_tfidf_embedding_array,\n",
    "                                                                                                  mode)\n",
    "                print i, mode\n",
    "\n",
    "                if i==0:\n",
    "                    name = \"chars_5gram_tfidf_SVD_char_embed_to_sentence_embed\"\n",
    "                else:\n",
    "                    name = \"word2vec_char_embed_to_sentence_embed\"\n",
    "                np.save(name+\"_mode_\"+mode,question_emb_from_word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs = DataSet.load_all_questions()\n",
    "words_corpus = DataSet.load_corpus(space=\"words\")\n",
    "chars_corpus = DataSet.load_corpus(space=\"chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = chars_corpus\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,lowercase=False)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "nmf = NMF(n_components=50, \n",
    "          alpha=.1, l1_ratio=.5).fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728428, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "np.save(\"chars_1gram_tfidf_NMF_sentence_embed\",nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_word = np.load(\"words_1gram_tfidf_NMF_sentence_embed.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.17349997e-03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  9.28468796e-03,   0.00000000e+00,   6.85196180e-05, ...,\n",
       "          2.52737415e-03,   0.00000000e+00,   1.51883866e-02],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  4.83890139e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_samples = chars_corpus\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,lowercase=False)\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "lda2 = LatentDirichletAllocation(n_components=30,learning_method='batch',n_jobs=-1).fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728428, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"chars_1gram_tf_LDA_sentence_embed\",lda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
